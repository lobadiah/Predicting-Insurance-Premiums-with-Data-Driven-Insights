{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29554069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "\n",
    "# GitHub Copilot\n",
    "# Insurance premium prediction - end-to-end notebook cell\n",
    "# Assumes \"Insurance Premium Prediction Dataset.csv\" is available in working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a475d3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows, cols: (278860, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Annual Income</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Number of Dependents</th>\n",
       "      <th>Education Level</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Health Score</th>\n",
       "      <th>Location</th>\n",
       "      <th>Policy Type</th>\n",
       "      <th>Previous Claims</th>\n",
       "      <th>Vehicle Age</th>\n",
       "      <th>Credit Score</th>\n",
       "      <th>Insurance Duration</th>\n",
       "      <th>Premium Amount</th>\n",
       "      <th>Policy Start Date</th>\n",
       "      <th>Customer Feedback</th>\n",
       "      <th>Smoking Status</th>\n",
       "      <th>Exercise Frequency</th>\n",
       "      <th>Property Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>99990.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Master's</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.074627</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Comprehensive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>320.0</td>\n",
       "      <td>5</td>\n",
       "      <td>308.0</td>\n",
       "      <td>2022-12-10 15:21:39.078837</td>\n",
       "      <td>Poor</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Daily</td>\n",
       "      <td>Condo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>2867.0</td>\n",
       "      <td>Single</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.271335</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Comprehensive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>694.0</td>\n",
       "      <td>4</td>\n",
       "      <td>517.0</td>\n",
       "      <td>2023-01-31 15:21:39.078837</td>\n",
       "      <td>Good</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>30154.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.714909</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>Comprehensive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16</td>\n",
       "      <td>652.0</td>\n",
       "      <td>8</td>\n",
       "      <td>849.0</td>\n",
       "      <td>2023-11-26 15:21:39.078837</td>\n",
       "      <td>Poor</td>\n",
       "      <td>No</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>48371.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>0.0</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>25.346926</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Comprehensive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>330.0</td>\n",
       "      <td>7</td>\n",
       "      <td>927.0</td>\n",
       "      <td>2023-02-27 15:21:39.078837</td>\n",
       "      <td>Poor</td>\n",
       "      <td>No</td>\n",
       "      <td>Rarely</td>\n",
       "      <td>Condo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>54174.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>0.0</td>\n",
       "      <td>High School</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>6.659499</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Comprehensive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2020-11-25 15:21:39.078837</td>\n",
       "      <td>Poor</td>\n",
       "      <td>No</td>\n",
       "      <td>Rarely</td>\n",
       "      <td>Condo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Gender  Annual Income Marital Status  Number of Dependents  \\\n",
       "0  56.0    Male        99990.0        Married                   1.0   \n",
       "1  46.0    Male         2867.0         Single                   1.0   \n",
       "2  32.0  Female        30154.0       Divorced                   3.0   \n",
       "3  60.0  Female        48371.0       Divorced                   0.0   \n",
       "4  25.0  Female        54174.0       Divorced                   0.0   \n",
       "\n",
       "  Education Level     Occupation  Health Score  Location    Policy Type  \\\n",
       "0        Master's            NaN     31.074627     Urban  Comprehensive   \n",
       "1      Bachelor's            NaN     50.271335     Urban  Comprehensive   \n",
       "2      Bachelor's            NaN     14.714909  Suburban  Comprehensive   \n",
       "3             PhD  Self-Employed     25.346926     Rural  Comprehensive   \n",
       "4     High School  Self-Employed      6.659499     Urban  Comprehensive   \n",
       "\n",
       "   Previous Claims  Vehicle Age  Credit Score  Insurance Duration  \\\n",
       "0              NaN           13         320.0                   5   \n",
       "1              NaN            3         694.0                   4   \n",
       "2              2.0           16         652.0                   8   \n",
       "3              1.0           11         330.0                   7   \n",
       "4              NaN            9           NaN                   8   \n",
       "\n",
       "   Premium Amount           Policy Start Date Customer Feedback  \\\n",
       "0           308.0  2022-12-10 15:21:39.078837              Poor   \n",
       "1           517.0  2023-01-31 15:21:39.078837              Good   \n",
       "2           849.0  2023-11-26 15:21:39.078837              Poor   \n",
       "3           927.0  2023-02-27 15:21:39.078837              Poor   \n",
       "4           303.0  2020-11-25 15:21:39.078837              Poor   \n",
       "\n",
       "  Smoking Status Exercise Frequency Property Type  \n",
       "0            Yes              Daily         Condo  \n",
       "1            Yes            Monthly         House  \n",
       "2             No            Monthly         House  \n",
       "3             No             Rarely         Condo  \n",
       "4             No             Rarely         Condo  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewed numeric columns to log1p: ['Previous Claims']\n",
      "Training RandomForest...\n",
      "Training GradientBoosting...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nGradientBoostingRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 111\u001b[0m\n\u001b[0;32m    109\u001b[0m rf_pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining GradientBoosting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 111\u001b[0m gbr_pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# 9) Evaluation helper\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_model\u001b[39m(pipe, X_tr, X_te, y_tr, y_te, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:662\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    657\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    658\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    659\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    660\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    661\u001b[0m         )\n\u001b[1;32m--> 662\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:658\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_state()\n\u001b[0;32m    654\u001b[0m \u001b[38;5;66;03m# Check input\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;66;03m# Since check_array converts both X and y to the same dtype, but the\u001b[39;00m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;66;03m# trees use different types for X and y, checking them separately.\u001b[39;00m\n\u001b[1;32m--> 658\u001b[0m X, y \u001b[38;5;241m=\u001b[39m validate_data(\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    660\u001b[0m     X,\n\u001b[0;32m    661\u001b[0m     y,\n\u001b[0;32m    662\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    663\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mDTYPE,\n\u001b[0;32m    664\u001b[0m     multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    665\u001b[0m )\n\u001b[0;32m    666\u001b[0m sample_weight_is_none \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    667\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1370\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[0;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[1;32m-> 1370\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1371\u001b[0m     X,\n\u001b[0;32m   1372\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1373\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1374\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1375\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1376\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1377\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39mforce_writeable,\n\u001b[0;32m   1378\u001b[0m     ensure_all_finite\u001b[38;5;241m=\u001b[39mensure_all_finite,\n\u001b[0;32m   1379\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1380\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1381\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1382\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1383\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1384\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1385\u001b[0m )\n\u001b[0;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1389\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m-> 1107\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1108\u001b[0m         array,\n\u001b[0;32m   1109\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1110\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1111\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mensure_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1112\u001b[0m     )\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    121\u001b[0m     X,\n\u001b[0;32m    122\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    123\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    124\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    125\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    126\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    127\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nGradientBoostingRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1) Load data\n",
    "df = pd.read_csv(r\"C:\\Users\\Mr. Louis Obadiah\\Desktop\\OKAN\\Machine Learning\\The Projects\\Predicting Insurance Premiums with Data-Driven Insights for SecureLife Insurance Co\\Insurance Premium Prediction Dataset.csv\")\n",
    "\n",
    "\n",
    "# Quick look\n",
    "print(\"Rows, cols:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "# 2) Basic cleanup & type corrections\n",
    "# Parse Policy Start Date and create policy age in years\n",
    "df['Policy Start Date'] = pd.to_datetime(df['Policy Start Date'], errors='coerce')\n",
    "current_date = pd.to_datetime(\"today\")\n",
    "df['Policy_Age_Years'] = ((current_date - df['Policy Start Date']).dt.days / 365.25).fillna(0).clip(lower=0)\n",
    "\n",
    "# Simple text feature: length of feedback\n",
    "df['Feedback_Len'] = df['Customer Feedback'].fillna(\"\").astype(str).map(len)\n",
    "\n",
    "# Map binary and ordinal fields\n",
    "if 'Smoking Status' in df.columns:\n",
    "    df['Smoking Status'] = df['Smoking Status'].map({'Yes': 1, 'No': 0})\n",
    "# Ordinal for Exercise Frequency\n",
    "exercise_order = ['Rarely', 'Monthly', 'Weekly', 'Daily']\n",
    "if 'Exercise Frequency' in df.columns:\n",
    "    df['Exercise Frequency'] = pd.Categorical(df['Exercise Frequency'], categories=exercise_order, ordered=True)\n",
    "\n",
    "# 3) Target and features\n",
    "target = 'Premium Amount'\n",
    "y = df[target].copy()\n",
    "X = df.drop(columns=[target])\n",
    "\n",
    "# Remove rows where target is NaN\n",
    "mask = ~y.isna()\n",
    "X = X.loc[mask].reset_index(drop=True)\n",
    "y = y.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# 4) Identify numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# remove Policy_Age_Years if accidentally in numeric list (we want to keep it though)\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Remove text columns that were transformed / redundant\n",
    "for col in ['Customer Feedback', 'Policy Start Date']:\n",
    "    if col in categorical_features:\n",
    "        categorical_features.remove(col)\n",
    "\n",
    "# 5) Handle skewed numeric columns by applying log1p transformer where appropriate\n",
    "skewed = X[numeric_features].skew().abs()\n",
    "skewed_cols = skewed[skewed > 1].index.tolist()  # threshold for skew\n",
    "print(\"Skewed numeric columns to log1p:\", skewed_cols)\n",
    "\n",
    "def log_transform(df_in):\n",
    "    df_out = df_in.copy()\n",
    "    for c in skewed_cols:\n",
    "        if c in df_out.columns:\n",
    "            df_out[c] = np.log1p(df_out[c].astype(float))\n",
    "    return df_out\n",
    "\n",
    "log_transformer = FunctionTransformer(log_transform)\n",
    "\n",
    "# 6) Preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('log', log_transformer, numeric_features),  # apply log1p then numeric pipeline\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "], remainder='drop')\n",
    "\n",
    "# Note: The above applies numeric features twice (log and num). To avoid duplication, we'll build a simple custom pipeline:\n",
    "# Instead, create a combined pipeline that first applies log transform to skewed numeric cols only, then imputes/scales all numeric.\n",
    "def make_preprocessor(numeric_features, skewed_cols, categorical_features):\n",
    "    # Custom transformer using ColumnTransformer properly\n",
    "    transformers = []\n",
    "    if len(skewed_cols) > 0:\n",
    "        transformers.append(('skewed_log', FunctionTransformer(\n",
    "            lambda df_in: df_in.assign(**{c: np.log1p(df_in[c].astype(float)) for c in skewed_cols}), validate=False),\n",
    "            skewed_cols))\n",
    "    # For numeric imputing/scaling (apply to all numeric)\n",
    "    transformers.append(('num', numeric_transformer, numeric_features))\n",
    "    # Categorical\n",
    "    transformers.append(('cat', categorical_transformer, categorical_features))\n",
    "    return ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "\n",
    "preprocessor = make_preprocessor(numeric_features, skewed_cols, categorical_features)\n",
    "\n",
    "# 7) Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 8) Build a baseline pipeline with RandomForest and one with GradientBoosting\n",
    "rf_pipeline = Pipeline(steps=[('pre', preprocessor),\n",
    "                              ('est', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))])\n",
    "\n",
    "gbr_pipeline = Pipeline(steps=[('pre', preprocessor),\n",
    "                               ('est', GradientBoostingRegressor(n_estimators=100, random_state=42))])\n",
    "\n",
    "# Fit baseline models\n",
    "print(\"Training RandomForest...\")\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "print(\"Training GradientBoosting...\")\n",
    "gbr_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 9) Evaluation helper\n",
    "def evaluate_model(pipe, X_tr, X_te, y_tr, y_te, name=\"Model\"):\n",
    "    y_pred_tr = pipe.predict(X_tr)\n",
    "    y_pred_te = pipe.predict(X_te)\n",
    "    metrics = {\n",
    "        'train_mae': mean_absolute_error(y_tr, y_pred_tr),\n",
    "        'test_mae': mean_absolute_error(y_te, y_pred_te),\n",
    "        'test_mse': mean_squared_error(y_te, y_pred_te),\n",
    "        'test_rmse': mean_squared_error(y_te, y_pred_te, squared=False),\n",
    "        'test_r2': r2_score(y_te, y_pred_te)\n",
    "    }\n",
    "    print(f\"\\n{name} metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "rf_metrics = evaluate_model(rf_pipeline, X_train, X_test, y_train, y_test, \"RandomForest\")\n",
    "gbr_metrics = evaluate_model(gbr_pipeline, X_train, X_test, y_train, y_test, \"GradientBoosting\")\n",
    "\n",
    "# 10) Feature importance extraction for RandomForest\n",
    "# To get feature names after preprocessing:\n",
    "pre = rf_pipeline.named_steps['pre']\n",
    "# numeric names (as-is)\n",
    "num_names = numeric_features\n",
    "# categorical names from onehot\n",
    "cat_names = []\n",
    "if categorical_features:\n",
    "    # find the 'cat' transformer inside ColumnTransformer\n",
    "    for name, trans, cols in pre.transformers_:\n",
    "        if name == 'cat':\n",
    "            onehot = trans.named_steps['onehot']\n",
    "            # get feature names\n",
    "            cat_names = list(onehot.get_feature_names_out(cols))\n",
    "            break\n",
    "\n",
    "feature_names = num_names + cat_names\n",
    "# Sometimes skewed_log transformer may reorder; try to fall back on numeric features only if mismatch\n",
    "try:\n",
    "    importances = rf_pipeline.named_steps['est'].feature_importances_\n",
    "    if len(importances) == len(feature_names):\n",
    "        feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False).head(20)\n",
    "    else:\n",
    "        # fallback: show top numeric importances\n",
    "        feat_imp = pd.Series(importances, index=[f\"f_{i}\" for i in range(len(importances))]).sort_values(ascending=False).head(20)\n",
    "except Exception:\n",
    "    feat_imp = pd.Series(dtype=float)\n",
    "\n",
    "print(\"\\nTop feature importances (RandomForest):\")\n",
    "display(feat_imp)\n",
    "\n",
    "# 11) Simple hyperparameter tuning for RandomForest using RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'est__n_estimators': [100, 200, 400],\n",
    "    'est__max_depth': [None, 10, 20, 30],\n",
    "    'est__min_samples_split': [2, 5, 10],\n",
    "    'est__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "rs = RandomizedSearchCV(rf_pipeline, param_distributions=param_dist, n_iter=8, cv=3, scoring='neg_mean_absolute_error',\n",
    "                        random_state=42, n_jobs=-1, verbose=0)\n",
    "print(\"Running RandomizedSearchCV for RandomForest (this may take a while)...\")\n",
    "rs.fit(X_train, y_train)\n",
    "print(\"Best params:\", rs.best_params_)\n",
    "best_rf = rs.best_estimator_\n",
    "best_metrics = evaluate_model(best_rf, X_train, X_test, y_train, y_test, \"Tuned RandomForest\")\n",
    "\n",
    "# 12) Quick EDA plots (optional, comment/uncomment as needed)\n",
    "try:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.histplot(y, bins=50, kde=True)\n",
    "    plt.title(\"Premium Amount distribution\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(df.select_dtypes(include=[np.number]).corr(), cmap='coolwarm', center=0)\n",
    "    plt.title(\"Numeric features correlation\")\n",
    "    plt.show()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 13) Actionable Insights (printed summary)\n",
    "print(\"\\nActionable insights:\")\n",
    "print(\"- Review highly important features above to craft underwriting rules.\")\n",
    "print(\"- Consider log-transforming highly skewed monetary features (done automatically).\")\n",
    "print(\"- Investigate outliers in 'Previous Claims' and 'Annual Income' as they can skew premiums.\")\n",
    "print(\"- Use tuned RandomForest for deployment but validate with a hold-out or time-based split before production.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
