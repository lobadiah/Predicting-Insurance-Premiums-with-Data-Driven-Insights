{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29554069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "\n",
    "# GitHub Copilot\n",
    "# Insurance premium prediction - end-to-end notebook cell\n",
    "# Assumes \"Insurance Premium Prediction Dataset.csv\" is available in working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a475d3a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'warnings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[1;32m----> 2\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 1) Load data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMr. Louis Obadiah\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOKAN\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMachine Learning\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mThe Projects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPredicting Insurance Premiums with Data-Driven Insights for SecureLife Insurance Co\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mInsurance Premium Prediction Dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'warnings' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1) Load data\n",
    "df = pd.read_csv(r\"C:\\Users\\Mr. Louis Obadiah\\Desktop\\OKAN\\Machine Learning\\The Projects\\Predicting Insurance Premiums with Data-Driven Insights for SecureLife Insurance Co\\Insurance Premium Prediction Dataset.csv\")\n",
    "\n",
    "\n",
    "# Quick look\n",
    "print(\"Rows, cols:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "# 2) Basic cleanup & type corrections\n",
    "# Parse Policy Start Date and create policy age in years\n",
    "df['Policy Start Date'] = pd.to_datetime(df['Policy Start Date'], errors='coerce')\n",
    "current_date = pd.to_datetime(\"today\")\n",
    "df['Policy_Age_Years'] = ((current_date - df['Policy Start Date']).dt.days / 365.25).fillna(0).clip(lower=0)\n",
    "\n",
    "# Simple text feature: length of feedback\n",
    "df['Feedback_Len'] = df['Customer Feedback'].fillna(\"\").astype(str).map(len)\n",
    "\n",
    "# Map binary and ordinal fields\n",
    "if 'Smoking Status' in df.columns:\n",
    "    df['Smoking Status'] = df['Smoking Status'].map({'Yes': 1, 'No': 0})\n",
    "# Ordinal for Exercise Frequency\n",
    "exercise_order = ['Rarely', 'Monthly', 'Weekly', 'Daily']\n",
    "if 'Exercise Frequency' in df.columns:\n",
    "    df['Exercise Frequency'] = pd.Categorical(df['Exercise Frequency'], categories=exercise_order, ordered=True)\n",
    "\n",
    "# 3) Target and features\n",
    "target = 'Premium Amount'\n",
    "y = df[target].copy()\n",
    "X = df.drop(columns=[target])\n",
    "\n",
    "# Remove rows where target is NaN\n",
    "mask = ~y.isna()\n",
    "X = X.loc[mask].reset_index(drop=True)\n",
    "y = y.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# 4) Identify numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# remove Policy_Age_Years if accidentally in numeric list (we want to keep it though)\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Remove text columns that were transformed / redundant\n",
    "for col in ['Customer Feedback', 'Policy Start Date']:\n",
    "    if col in categorical_features:\n",
    "        categorical_features.remove(col)\n",
    "\n",
    "# 5) Handle skewed numeric columns by applying log1p transformer where appropriate\n",
    "skewed = X[numeric_features].skew().abs()\n",
    "skewed_cols = skewed[skewed > 1].index.tolist()  # threshold for skew\n",
    "print(\"Skewed numeric columns to log1p:\", skewed_cols)\n",
    "\n",
    "def log_transform(df_in):\n",
    "    df_out = df_in.copy()\n",
    "    for c in skewed_cols:\n",
    "        if c in df_out.columns:\n",
    "            df_out[c] = np.log1p(df_out[c].astype(float))\n",
    "    return df_out\n",
    "\n",
    "log_transformer = FunctionTransformer(log_transform)\n",
    "\n",
    "# 6) Preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('log', log_transformer, numeric_features),  # apply log1p then numeric pipeline\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "], remainder='drop')\n",
    "\n",
    "# Note: The above applies numeric features twice (log and num). To avoid duplication, we'll build a simple custom pipeline:\n",
    "# Instead, create a combined pipeline that first applies log transform to skewed numeric cols only, then imputes/scales all numeric.\n",
    "def make_preprocessor(numeric_features, skewed_cols, categorical_features):\n",
    "    # Custom transformer using ColumnTransformer properly\n",
    "    transformers = []\n",
    "    if len(skewed_cols) > 0:\n",
    "        transformers.append(('skewed_log', FunctionTransformer(\n",
    "            lambda df_in: df_in.assign(**{c: np.log1p(df_in[c].astype(float)) for c in skewed_cols}), validate=False),\n",
    "            skewed_cols))\n",
    "    # For numeric imputing/scaling (apply to all numeric)\n",
    "    transformers.append(('num', numeric_transformer, numeric_features))\n",
    "    # Categorical\n",
    "    transformers.append(('cat', categorical_transformer, categorical_features))\n",
    "    return ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "\n",
    "preprocessor = make_preprocessor(numeric_features, skewed_cols, categorical_features)\n",
    "\n",
    "# 7) Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 8) Build a baseline pipeline with RandomForest and one with GradientBoosting\n",
    "rf_pipeline = Pipeline(steps=[('pre', preprocessor),\n",
    "                              ('est', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))])\n",
    "\n",
    "gbr_pipeline = Pipeline(steps=[('pre', preprocessor),\n",
    "                               ('est', GradientBoostingRegressor(n_estimators=100, random_state=42))])\n",
    "\n",
    "# Fit baseline models\n",
    "print(\"Training RandomForest...\")\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "print(\"Training GradientBoosting...\")\n",
    "gbr_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 9) Evaluation helper\n",
    "def evaluate_model(pipe, X_tr, X_te, y_tr, y_te, name=\"Model\"):\n",
    "    y_pred_tr = pipe.predict(X_tr)\n",
    "    y_pred_te = pipe.predict(X_te)\n",
    "    metrics = {\n",
    "        'train_mae': mean_absolute_error(y_tr, y_pred_tr),\n",
    "        'test_mae': mean_absolute_error(y_te, y_pred_te),\n",
    "        'test_mse': mean_squared_error(y_te, y_pred_te),\n",
    "        'test_rmse': mean_squared_error(y_te, y_pred_te, squared=False),\n",
    "        'test_r2': r2_score(y_te, y_pred_te)\n",
    "    }\n",
    "    print(f\"\\n{name} metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "rf_metrics = evaluate_model(rf_pipeline, X_train, X_test, y_train, y_test, \"RandomForest\")\n",
    "gbr_metrics = evaluate_model(gbr_pipeline, X_train, X_test, y_train, y_test, \"GradientBoosting\")\n",
    "\n",
    "# 10) Feature importance extraction for RandomForest\n",
    "# To get feature names after preprocessing:\n",
    "pre = rf_pipeline.named_steps['pre']\n",
    "# numeric names (as-is)\n",
    "num_names = numeric_features\n",
    "# categorical names from onehot\n",
    "cat_names = []\n",
    "if categorical_features:\n",
    "    # find the 'cat' transformer inside ColumnTransformer\n",
    "    for name, trans, cols in pre.transformers_:\n",
    "        if name == 'cat':\n",
    "            onehot = trans.named_steps['onehot']\n",
    "            # get feature names\n",
    "            cat_names = list(onehot.get_feature_names_out(cols))\n",
    "            break\n",
    "\n",
    "feature_names = num_names + cat_names\n",
    "# Sometimes skewed_log transformer may reorder; try to fall back on numeric features only if mismatch\n",
    "try:\n",
    "    importances = rf_pipeline.named_steps['est'].feature_importances_\n",
    "    if len(importances) == len(feature_names):\n",
    "        feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False).head(20)\n",
    "    else:\n",
    "        # fallback: show top numeric importances\n",
    "        feat_imp = pd.Series(importances, index=[f\"f_{i}\" for i in range(len(importances))]).sort_values(ascending=False).head(20)\n",
    "except Exception:\n",
    "    feat_imp = pd.Series(dtype=float)\n",
    "\n",
    "print(\"\\nTop feature importances (RandomForest):\")\n",
    "display(feat_imp)\n",
    "\n",
    "# 11) Simple hyperparameter tuning for RandomForest using RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'est__n_estimators': [100, 200, 400],\n",
    "    'est__max_depth': [None, 10, 20, 30],\n",
    "    'est__min_samples_split': [2, 5, 10],\n",
    "    'est__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "rs = RandomizedSearchCV(rf_pipeline, param_distributions=param_dist, n_iter=8, cv=3, scoring='neg_mean_absolute_error',\n",
    "                        random_state=42, n_jobs=-1, verbose=0)\n",
    "print(\"Running RandomizedSearchCV for RandomForest (this may take a while)...\")\n",
    "rs.fit(X_train, y_train)\n",
    "print(\"Best params:\", rs.best_params_)\n",
    "best_rf = rs.best_estimator_\n",
    "best_metrics = evaluate_model(best_rf, X_train, X_test, y_train, y_test, \"Tuned RandomForest\")\n",
    "\n",
    "# 12) Quick EDA plots (optional, comment/uncomment as needed)\n",
    "try:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.histplot(y, bins=50, kde=True)\n",
    "    plt.title(\"Premium Amount distribution\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(df.select_dtypes(include=[np.number]).corr(), cmap='coolwarm', center=0)\n",
    "    plt.title(\"Numeric features correlation\")\n",
    "    plt.show()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 13) Actionable Insights (printed summary)\n",
    "print(\"\\nActionable insights:\")\n",
    "print(\"- Review highly important features above to craft underwriting rules.\")\n",
    "print(\"- Consider log-transforming highly skewed monetary features (done automatically).\")\n",
    "print(\"- Investigate outliers in 'Previous Claims' and 'Annual Income' as they can skew premiums.\")\n",
    "print(\"- Use tuned RandomForest for deployment but validate with a hold-out or time-based split before production.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
